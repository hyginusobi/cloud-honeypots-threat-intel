# Logging and Collection

## Purpose
This document explains how logs generated by the honeypot environment are collected, centralised, and prepared for analysis.

Effective logging is critical to:
- Understand attacker behaviour
- Support SOC-style investigations
- Generate threat intelligence
- Preserve evidence for analysis and reporting

The design focuses on **visibility, reliability, and safety**.

---

## Logging Strategy Overview
The logging strategy follows these principles:

- Capture all relevant attacker interactions
- Centralise logs for correlation and analysis
- Preserve timestamps and context
- Avoid collecting unnecessary or sensitive data
- Ensure logs cannot be tampered with by attackers

Logs are treated as **security evidence**.

---

## Log Sources
The honeypot environment generates multiple log sources, including:

### 1. System Logs
- Authentication attempts
- User and service activity
- Privilege escalation attempts
- Process execution events

These logs help identify brute-force attacks, persistence attempts, and suspicious commands.

---

### 2. Honeypot Application Logs
Depending on the honeypot services deployed, logs may include:
- Connection metadata (source IP, port, protocol)
- Commands issued by attackers
- Payloads or probes sent to services
- Session durations and behaviour patterns

These logs are the primary source of threat intelligence.

---

### 3. Network-Level Logs
Where enabled, network logs may include:
- Connection attempts
- Port scanning activity
- Unusual traffic patterns
- Repeated access attempts from the same source

Network logs support correlation with host and application events.

---

## Centralised Log Collection
Logs are forwarded from the honeypot host(s) to a central collection point.

Key design considerations:
- Logs are forwarded off the honeypot host to reduce tampering risk
- Secure transport mechanisms are used where possible
- Log forwarding is automated to avoid manual intervention

Centralisation enables:
- Cross-log correlation
- Timeline reconstruction
- Efficient querying and analysis

---

## Log Normalisation & Parsing
To support effective analysis:
- Logs are normalised into a consistent format
- Key fields such as timestamps, source IPs, ports, and actions are extracted
- Parsing enables efficient searching and aggregation

This step is essential for SOC-style workflows and reporting.

---

## Retention & Storage
Retention decisions balance:
- Cost
- Storage capacity
- Investigation needs

Recommended approach:
- Retain raw logs for a defined short-term period
- Retain aggregated or summarised data for longer-term analysis
- Ensure logs are protected against deletion or modification

---

## Security Controls for Logging
Logging infrastructure is protected through:
- Restricted access to log storage
- Role-based access controls
- Monitoring of log pipeline health
- Separation of logging systems from honeypot hosts

These controls ensure the integrity and availability of security data.

---

## Validation Checks
Before considering logging complete, verify:

- [ ] Logs are being generated on the honeypot host
- [ ] Logs are successfully forwarded to the central collection point
- [ ] Timestamps are accurate and consistent
- [ ] No sensitive data is unintentionally captured
- [ ] Log ingestion remains stable under attack activity
- [ ] Logs are accessible for analysis

---

## Operational Notes
From an operational perspective:
- Monitor log volume to detect spikes in activity
- Watch for log pipeline failures
- Adjust retention policies as activity increases
- Document changes to logging configuration

Logging is not static â€” it evolves with attacker behaviour.

---

## Next Step
Once logging is confirmed, proceed to analysing and detecting attacker behaviour:

- `detection-and-logging/log-sources.md`

