# ELK Stack Overview

## Purpose
This document explains how the **ELK Stack (Elasticsearch, Logstash, Kibana)** is used to ingest, store, analyse, and visualise logs generated by the honeypot environment.

The ELK Stack enables:
- Centralised log storage
- Efficient searching and filtering
- Correlation of attacker activity
- Visualisation of trends and patterns
- SOC-style investigation workflows

---

## ELK Stack Components
The logging and analysis pipeline consists of the following components:

### Elasticsearch
Elasticsearch is used as the primary data store for log data.

Responsibilities include:
- Indexing structured and semi-structured logs
- Supporting fast search and aggregation
- Retaining historical security events
- Enabling correlation across log sources

Elasticsearch forms the backbone of the analysis layer.

---

### Logstash
Logstash is responsible for log ingestion and processing.

Key functions:
- Receiving logs from honeypot hosts
- Parsing and normalising log fields
- Enriching events where appropriate
- Forwarding processed logs to Elasticsearch

Logstash ensures logs are consistent and usable for analysis.

---

### Kibana
Kibana provides the visual interface for analysis and investigation.

Capabilities include:
- Interactive dashboards
- Time-based event analysis
- Visualisation of attack trends
- Filtering by IP address, port, service, or time window
- Supporting ad-hoc investigation queries

Kibana is the primary tool used by analysts during investigations.

---

## Log Ingestion Flow
The log ingestion process follows this flow:

1. Honeypot services generate logs
2. Logs are forwarded from the honeypot host
3. Logstash processes and normalises events
4. Elasticsearch indexes the data
5. Kibana visualises and queries the logs

This flow ensures that attacker activity is captured end-to-end.

---

## Data Normalisation
Normalisation is critical for meaningful analysis.

Examples of normalised fields:
- Timestamp
- Source IP address (sanitised)
- Destination port
- Service name
- Action or outcome
- Event type

Normalisation enables correlation across different log sources and services.

---

## Dashboards & Visualisations
Dashboards are designed to highlight:
- Top source IPs by activity
- Most targeted services and ports
- Authentication failure trends
- Temporal patterns (time of day, frequency)
- Spikes in scanning or brute-force activity

Dashboards provide both **high-level situational awareness** and **deep investigative capability**.

---

## SOC Investigation Use Cases
The ELK Stack supports common SOC use cases, including:
- Identifying repeated attack attempts from a single source
- Distinguishing automated scanning from manual interaction
- Tracking attacker behaviour over time
- Validating alerts raised by detection logic
- Supporting evidence-based escalation decisions

---

## Performance & Stability Considerations
To maintain reliability:
- Monitor ingestion latency
- Watch index growth and storage usage
- Tune retention and rollover policies
- Validate pipeline health regularly

Stability ensures logs remain available during high activity periods.

---

## Security Controls
Security of the ELK environment includes:
- Restricted access to dashboards and indices
- Role-based access controls
- Separation from the honeypot hosts
- Monitoring of administrative access

These controls protect the integrity of security data.

---

## Limitations
Known limitations include:
- Resource constraints in small-scale deployments
- Manual tuning requirements for optimal performance
- Potential ingestion delays during traffic spikes

These limitations are acceptable within the scope of this project and documented for transparency.

---

## Next Step
Define how detection and alerting logic is applied to the data:

- `detection-and-logging/alerting-logic.md`

